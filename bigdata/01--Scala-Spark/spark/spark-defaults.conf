spark.password.factory = com.huawei.spark.client.ClientDecode
spark.rdd.compress = false
spark.network.timeout = 120s
spark.sql.shuffle.partitions = 200
spark.hbase.obtainToken.enabled = false
spark.storage.memoryMapThreshold = 2m
spark.yarn.jar = local:/opt/huawei/Bigdata/FusionInsight/spark/spark/lib/*
spark.shuffle.compress = true
spark.kryo.registrationRequired = false
spark.dynamicAllocation.minExecutors = 0
spark.task.maxFailures = 4
spark.locality.wait.rack = 3000
spark.logConf = false
spark.kryoserializer.buffer.max = 64MB
spark.akka.frameSize = 128
spark.executor.heartbeatInterval = 10000
spark.broadcast.blockSize = 4096
spark.yarn.am.extraJavaOptions = -Dlog4j.configuration=file:/opt/huawei/Bigdata/FusionInsight/spark/cfg/log4j-executor.properties
spark.storage.memoryFraction = 0.6
spark.shuffle.memoryFraction = 0.2
spark.speculation.multiplier = 1.5
spark.dynamicAllocation.enabled = false
spark.files.overwrite = false
spark.locality.wait.node = 3000
spark.yarn.cluster.driver.extraClassPath = /opt/huawei/Bigdata/FusionInsight/spark/cfg/:/opt/huawei/Bigdata/FusionInsight/spark/spark/lib/*
spark.driver.extraClassPath = /opt/hadoopclient/Spark/spark/conf/:/opt/hadoopclient/Spark/spark/lib/*
spark.sql.bigdata.register.dialect = org.apache.spark.sql.hbase.HBaseSQLParser
spark.hadoop.validateOutputSpecs = true
spark.shuffle.manager = SORT
spark.reducer.maxSizeInFlight = 48MB
spark.kryo.referenceTracking = true
spark.akka.heartbeat.pauses = 6000s
spark.scheduler.executorTaskBlacklistTime = 60000
spark.locality.wait = 3000
spark.dynamicAllocation.initialExecutors = 0
spark.akka.timeout = 100s
spark.dynamicAllocation.executorIdleTimeout = 60
spark.broadcast.factory = org.apache.spark.broadcast.TorrentBroadcastFactory
spark.storage.unrollFraction = 0.2
spark.executor.memory = 1G
spark.shuffle.sort.bypassMergeThreshold = 200
spark.dynamicAllocation.schedulerBacklogTimeout = 1
spark.scheduler.minRegisteredResourcesRatio = 0.8
spark.sql.bigdata.register.preExecutionRule = org.apache.spark.sql.hbase.execution.AddCoprocessor$,org.apache.spark.sql.execution.EnsureRowFormats$
spark.io.compression.lz4.blockSize = 32KB
spark.eventLog.compress = false
spark.port.maxRetries = 16
spark.scheduler.mode = FIFO
spark.akka.threads = 4
spark.serializer.objectStreamReset = 100
spark.shuffle.consolidateFiles = false
spark.yarn.am.memory = 512M
spark.executor.extraClassPath = /opt/huawei/Bigdata/FusionInsight/spark/cfg/:/opt/huawei/Bigdata/FusionInsight/spark/spark/lib/*
spark.executor.extraLibraryPath = /opt/huawei/Bigdata/hadoop/hadoop/lib/native
spark.shuffle.spill = true
spark.ui.customErrorPage = true
spark.beeline.principal = spark/hadoop.hadoop.com@HADOOP.COM
spark.locality.wait.process = 3000
spark.sql.bigdata.register.strategyRule = org.apache.spark.sql.hbase.DummySparkPlanner
spark.io.compression.snappy.blockSize = 32KB
spark.scheduler.maxRegisteredResourcesWaitingTime = 1800000
spark.sql.dialect = org.apache.spark.sql.hive.huawei.BigSQLDialect
spark.security.hideInfo.enabled = true
spark.yarn.cluster.driver.extraLibraryPath = /opt/huawei/Bigdata/hadoop/hadoop/lib/native
spark.driver.extraLibraryPath =/opt/hadoopclient/HDFS/hadoop/lib/native
spark.files.fetchTimeout = 60
spark.ui.port = 0
spark.ui.killEnabled = true
spark.io.compression.codec = snappy
spark.speculation = false
spark.sql.bigdata.register.extendedResolutionRule = org.apache.spark.sql.hbase.catalyst.analysis.ReplaceOutput$
spark.sql.bigdata.initFunction = org.apache.spark.sql.hbase.HBaseEnv
spark.eventQueue.size = 1000000
spark.ui.retainedStages = 1000
spark.yarn.cluster.driver.extraJavaOptions = -Xloggc:<LOG_DIR>/gc.log -XX:+PrintGCDetails -XX:-OmitStackTraceInFastThrow -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=20 -XX:GCLogFileSize=10M -Dlog4j.configuration=file:/opt/huawei/Bigdata/FusionInsight/spark/cfg/log4j-executor.properties -Djava.security.auth.login.config=/opt/huawei/Bigdata/FusionInsight/spark/cfg/jaas-zk.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Djava.security.krb5.conf=/opt/huawei/Bigdata/FusionInsight/spark/cfg/kdc.conf -Djetty.version=x.y.z -Dorg.xerial.snappy.tempdir=/opt/huawei/Bigdata/tmp -Djava.io.tmpdir=/opt/huawei/Bigdata/tmp
spark.driver.extraJavaOptions = -Dlog4j.configuration=file:/opt/hadoopclient/Spark/spark/conf/log4j.properties -Djetty.version=x.y.z -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Djava.security.krb5.conf=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf -Djava.security.auth.login.config=/opt/hadoopclient/Spark/adapter/client/controller/jaas.conf -Dorg.xerial.snappy.tempdir=/opt/hadoopclient/Spark/tmp
spark.eventLog.overwrite = true
spark.dynamicAllocation.cachedExecutorIdleTimeout = 120
spark.shuffle.spill.compress = true
spark.serializer = org.apache.spark.serializer.JavaSerializer
spark.eventLog.dir = hdfs://hacluster/sparkJobHistory
spark.kryoserializer.buffer = 64KB
spark.localExecution.enabled = false
spark.random.port.min = 23000
spark.dynamicAllocation.sustainedSchedulerBacklogTimeout = 1
spark.speculation.quantile = 0.75
custom = 
spark.executor.cores = 1
spark.acls.enable = false
spark.eventLog.group.size = 30
spark.dynamicAllocation.maxExecutors = 2147483647
spark.authenticate = true
spark.ui.threadDumpsEnabled = false
spark.task.cpus = 1
spark.random.port.max = 23999
spark.speculation.interval = 100
spark.shuffle.service.port = 27337
spark.inputFormat.cache.enabled = true
spark.shuffle.file.buffer = 32KB
spark.eventLog.enabled = true
spark.shuffle.service.enabled = false
spark.executor.extraJavaOptions = -Xloggc:<LOG_DIR>/gc.log -XX:+PrintGCDetails -XX:-OmitStackTraceInFastThrow -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=20 -XX:GCLogFileSize=10M -Dlog4j.configuration=file:/opt/huawei/Bigdata/FusionInsight/spark/cfg/log4j-executor.properties -Djava.security.auth.login.config=/opt/huawei/Bigdata/FusionInsight/spark/cfg/jaas-zk.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Djava.security.krb5.conf=/opt/huawei/Bigdata/FusionInsight/spark/cfg/kdc.conf
spark.sql.authorization.enabled = true
spark.scheduler.revive.interval = 1000
spark.local.dir = /tmp
spark.authenticate.secret = 
hadoop_server_path = /opt/huawei/Bigdata/hadoop/
